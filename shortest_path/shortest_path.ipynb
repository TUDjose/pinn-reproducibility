{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(layers[i],layers[i+1]) for i in range(len(layers)-1)\n",
    "        ])\n",
    "        self.activation = F.sigmoid\n",
    "        for layer in self.linears:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears[:-2]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefractionPINN(FNN):\n",
    "    def __init__(self, n_output):\n",
    "        super().__init__([1]+[64]*3+[n_output])\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_loss(x, y, t, save_loss=True):\n",
    "       \n",
    "        x_t = torch.autograd.grad(x, t, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "        y_t = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "\n",
    "        L_con = 0\n",
    "        if t[0] == tmin:\n",
    "            L_con = ((x[0]-x0)**2+(y[0] - y0)**2+(x[-1] - x1)**2+(y[-1] - y1)**2)\n",
    "        refraction = n1 + (n2 - n1) * 0.5 * (1. - torch.cos(2. * np.pi * y))\n",
    "        L_phys = ((x_t / T)**2 +(y_t / T)**2 - (c0 / refraction)**2).abs().square().mean()\n",
    "\n",
    "        L_goal = 0\n",
    "        #if t[-1] == tmax:\n",
    "        L_goal = T\n",
    "        L_con = loss_weights[0]*L_con\n",
    "        L_phys = loss_weights[1]*L_phys\n",
    "        L_goal = loss_weights[2]*L_goal\n",
    "        #print(f\"con: {L_con}, phys: {L_phys}, goal: {L_goal}\")\n",
    "        if save_loss:\n",
    "            losses.append((L_con,L_phys,L_goal))\n",
    "        return L_con + L_phys + L_goal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points():\n",
    "    points = [tmin]\n",
    "    points += [random.uniform(tmin,tmax) for _ in range(num_domain-2)]\n",
    "    points += [tmax]\n",
    "    points.sort()\n",
    "\n",
    "    points_tensor = torch.tensor(points).reshape((num_domain, 1))\n",
    "    points_tensor.requires_grad = True\n",
    "    return points_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, steps):\n",
    "    model.train()\n",
    "    time_dom = sample_points()\n",
    "    #print(f\"...Adam\")\n",
    "    for n in range(n_adam):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if n % period:\n",
    "            time_dom = sample_points()\n",
    "        u = model(time_dom)\n",
    "        x, y = u[:,0], u[:,1]\n",
    "        loss = pinn_loss(x, y, time_dom)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if n % period == 0:\n",
    "        #    print(f\"ADAM Loss {n}/{n_adam}: \\n{loss}\\n_______________\")\n",
    "    def closure(time_dom, save_losses):\n",
    "        optimizer.zero_grad()\n",
    "        u = model(time_dom)\n",
    "        x, y = u[:,0], u[:,1]\n",
    "        loss = pinn_loss(x, y, time_dom)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    #print(\"...L-BFGS\")\n",
    "    optimizer = torch.optim.LBFGS(list(model.parameters()) + [T], lr = lr)\n",
    "    time_dom = torch.linspace(tmin,tmax,num_domain).reshape((num_domain,1))\n",
    "    time_dom.requires_grad = True\n",
    "    try:\n",
    "        for n in range(n_lbfgs):\n",
    "            optimizer.step(lambda: closure(time_dom,False))\n",
    "            loss = closure(time_dom, True)\n",
    "            #if n % period == 0:\n",
    "            #    print(f\"LBFGS Loss {n}/{n_lbfgs}: \\n{loss}\\n_______________\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(model):\n",
    "    model.eval()\n",
    "    time_dom = torch.linspace(tmin, tmax, num_domain).reshape((num_domain,1))\n",
    "    with torch.no_grad():\n",
    "        u = [model(t.reshape(1)).to(device=\"cpu\") for t in time_dom]\n",
    "    plt.subplot(1,2,1)\n",
    "    #plt.plot(time_dom.detach().numpy(), [y[0] for y in u], label = \"x\")\n",
    "    plt.plot([y[0] for y in u], [y[1] for y in u])\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([x[0].detach().numpy() for x in losses], label=\"constraint\")\n",
    "    plt.plot([x[1].detach().numpy() for x in losses], label=\"physics\")\n",
    "    plt.plot([x[2].detach().numpy() for x in losses], label=\"goal\")\n",
    "    plt.plot([x[0].detach().numpy() + x[1].detach().numpy() + x[2].detach().numpy() for x in losses], label = \"total loss\")\n",
    "    plt.semilogy()\n",
    "    plt.legend()\n",
    "    plt.suptitle(\"Learned function (left) and losses during training (right)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = 0.0, 1.0\n",
    "T = torch.tensor(1.0, requires_grad = True)\n",
    "x0, y0 = 0., 0.\n",
    "x1, y1 = 1., 1.\n",
    "c0, n1, n2 = 1., 1., 2.\n",
    "losses = []\n",
    "x_t = None\n",
    "y_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = 2\n",
    "num_domain = 1000 \n",
    "n_adam =  2000\n",
    "lr = 1e-3\n",
    "loss_weights = [1.,1.,0.01] \n",
    "period = 100\n",
    "n_lbfgs = 1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = RefractionPINN(n_output)\n",
    "net.to(device=\"cpu\")\n",
    "print(\"Training\")\n",
    "\n",
    "train(net, torch.optim.Adam(list(net.parameters())+ [T],lr = lr), n_adam)\n",
    "\n",
    "print(\"Plotting\")\n",
    "\n",
    "plot_output(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brachistochrone curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_loss_brach(x, y, t, save_loss=True):\n",
    "       \n",
    "        x_t = torch.autograd.grad(x, t, grad_outputs=torch.ones_like(x), create_graph=True)[0]\n",
    "        y_t = torch.autograd.grad(y, t, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "\n",
    "        L_con = 0\n",
    "        if t[0] == tmin:\n",
    "            L_con = ((x[0]-x0)**2+(y[0] - y0)**2+(x[-1] - x1)**2+(y[-1] - y1)**2)\n",
    "        \n",
    "        L_phys = (g*y0-(g*y+0.5*((x_t/T)**2+(y_t/T)**2))).abs().square().mean()\n",
    "\n",
    "        L_goal = 0\n",
    "        #if t[-1] == tmax:\n",
    "        L_goal = T\n",
    "        L_con = loss_weights[0]*L_con\n",
    "        L_phys = loss_weights[1]*L_phys\n",
    "        L_goal = loss_weights[2]*L_goal\n",
    "        #print(f\"con: {L_con}, phys: {L_phys}, goal: {L_goal}\")\n",
    "        if save_loss:\n",
    "            losses.append((L_con,L_phys,L_goal))\n",
    "        return L_con + L_phys + L_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBrach(model, optimizer, steps):\n",
    "    model.train()\n",
    "    time_dom = sample_points()\n",
    "    #print(f\"...Adam\")\n",
    "    for n in range(n_adam):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if n % period:\n",
    "            time_dom = sample_points()\n",
    "        u = model(time_dom)\n",
    "        x, y = u[:,0], u[:,1]\n",
    "        loss = pinn_loss_brach(x, y, time_dom)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if n % period == 0:\n",
    "        #    print(f\"ADAM Loss {n}/{n_adam}: \\n{loss}\\n_______________\")\n",
    "    def closure(time_dom, save_losses):\n",
    "        optimizer.zero_grad()\n",
    "        u = model(time_dom)\n",
    "        x, y = u[:,0], u[:,1]\n",
    "        loss = pinn_loss_brach(x, y, time_dom)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    #print(\"...L-BFGS\")\n",
    "    optimizer = torch.optim.LBFGS(list(model.parameters()) + [T], lr = lr)\n",
    "    time_dom = torch.linspace(tmin,tmax,num_domain).reshape((num_domain,1))\n",
    "    time_dom.requires_grad = True\n",
    "    try:\n",
    "        for n in range(n_lbfgs):\n",
    "            optimizer.step(lambda: closure(time_dom,False))\n",
    "            loss = closure(time_dom, True)\n",
    "            #if n % period == 0:\n",
    "            #    print(f\"LBFGS Loss {n}/{n_lbfgs}: \\n{loss}\\n_______________\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = 0.0, 1.0\n",
    "T = torch.tensor(1.0, requires_grad = True)\n",
    "x0, y0 = 0., 1.\n",
    "x1, y1 = 1., 0.\n",
    "c0, n1, n2 = 1., 1., 2.\n",
    "losses = []\n",
    "x_t = None\n",
    "y_t = None\n",
    "g = 9.8\n",
    "n_output = 2\n",
    "num_domain = 1000 \n",
    "n_adam =  2000\n",
    "lr = 1e-3\n",
    "loss_weights = [1.,1.,0.01] \n",
    "period = 100\n",
    "n_lbfgs = 2692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RefractionPINN(n_output)\n",
    "net.to(device=\"cpu\")\n",
    "print(\"Training\")\n",
    "\n",
    "trainBrach(net, torch.optim.Adam(list(net.parameters())+ [T],lr = lr), n_adam)\n",
    "\n",
    "print(\"Plotting\")\n",
    "\n",
    "plot_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = 0.0, 1.0\n",
    "\n",
    "x0, y0 = 0., 0.\n",
    "x1, y1 = 1., 1.\n",
    "c0, n1, n2 = 1., 1., 2.\n",
    "\n",
    "x_t = None\n",
    "y_t = None\n",
    "n_output = 2\n",
    "num_domain = 1000 \n",
    "\n",
    "n_adam =  2000\n",
    "lr = 1e-3\n",
    "lc = [1., 5.]\n",
    "lp = [1., 5.]\n",
    "lg = [0.01, 0.1]\n",
    "period = 100\n",
    "n_lbfgs = 1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lc:\n",
    "    for j in lp:\n",
    "        for k in lg:\n",
    "            T = torch.tensor(1.0, requires_grad = True)\n",
    "            losses = []\n",
    "            print(f\"Evaluating loss weights: {i}, {j}, {k} \")\n",
    "            loss_weights = [i,j,k]\n",
    "            net = RefractionPINN(n_output)\n",
    "            net.to(device=\"cpu\")\n",
    "            print(\"Training\")\n",
    "\n",
    "            train(net, torch.optim.Adam(list(net.parameters())+ [T],lr = lr), n_adam)\n",
    "\n",
    "            print(\"Plotting\")\n",
    "\n",
    "            net.eval()\n",
    "            time_dom = torch.linspace(tmin, tmax, num_domain).reshape((num_domain,1))\n",
    "            with torch.no_grad():\n",
    "                u = [net(t.reshape(1)).to(device=\"cpu\") for t in time_dom]\n",
    "            plt.subplot(1,2,1)\n",
    "            #plt.plot(time_dom.detach().numpy(), [y[0] for y in u], label = \"x\")\n",
    "            plt.plot([y[0] for y in u], [y[1] for y in u])\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"X\")\n",
    "            plt.ylabel(\"Y\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.ylim(0, 1)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot([x[0].detach().numpy() for x in losses], label=\"constraint\")\n",
    "            plt.plot([x[1].detach().numpy() for x in losses], label=\"physics\")\n",
    "            plt.plot([x[2].detach().numpy() for x in losses], label=\"goal\")\n",
    "            plt.plot([x[0].detach().numpy() + x[1].detach().numpy() + x[2].detach().numpy() for x in losses], label = \"total loss\")\n",
    "            plt.semilogy()\n",
    "            plt.legend()\n",
    "            plt.suptitle(f\"Constraint: {i}, Physics: {j}, Goal: {k}\")\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now for the shortest path\")\n",
    "tmin, tmax = 0.0, 1.0\n",
    "\n",
    "x0, y0 = 0., 0.\n",
    "x1, y1 = 1., 1.\n",
    "c0, n1, n2 = 1., 1., 2.\n",
    "\n",
    "x_t = None\n",
    "y_t = None\n",
    "n_output = 2\n",
    "num_domain = 1000 \n",
    "g = 9.8\n",
    "n_adam =  2000\n",
    "lr = 1e-3\n",
    "lc = [1., 5., 10.]\n",
    "lp = [1., 5., 10.]\n",
    "lg = [0.01, 0.1,0.5]\n",
    "period = 100\n",
    "n_lbfgs = 2692\n",
    "for i in lc:\n",
    "    for j in lp:\n",
    "        for k in lg:\n",
    "            T = torch.tensor(1.0, requires_grad = True)\n",
    "            losses = []\n",
    "            print(f\"Evaluating loss weights: {i}, {j}, {k} \")\n",
    "            loss_weights = [i,j,k]\n",
    "            net = RefractionPINN(n_output)\n",
    "            net.to(device=\"cpu\")\n",
    "            print(\"Training\")\n",
    "\n",
    "            trainBrach(net, torch.optim.Adam(list(net.parameters())+ [T],lr = lr), n_adam)\n",
    "\n",
    "            print(\"Plotting\")\n",
    "\n",
    "            net.eval()\n",
    "            time_dom = torch.linspace(tmin, tmax, num_domain).reshape((num_domain,1))\n",
    "            with torch.no_grad():\n",
    "                u = [net(t.reshape(1)).to(device=\"cpu\") for t in time_dom]\n",
    "            plt.subplot(1,2,1)\n",
    "            #plt.plot(time_dom.detach().numpy(), [y[0] for y in u], label = \"x\")\n",
    "            plt.plot([y[0] for y in u], [y[1] for y in u])\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"X\")\n",
    "            plt.ylabel(\"Y\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.ylim(0, 1)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot([x[0].detach().numpy() for x in losses], label=\"constraint\")\n",
    "            plt.plot([x[1].detach().numpy() for x in losses], label=\"physics\")\n",
    "            plt.plot([x[2].detach().numpy() for x in losses], label=\"goal\")\n",
    "            plt.plot([x[0].detach().numpy() + x[1].detach().numpy() + x[2].detach().numpy() for x in losses], label = \"total loss\")\n",
    "            plt.semilogy()\n",
    "            plt.legend()\n",
    "            plt.suptitle(f\"Constraint: {i}, Physics: {j}, Goal: {k}\")\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
